You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 01:45:48,686	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 01:45:48,687	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 18.83076 to 18.
2024-09-24 01:45:48,800	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/80 [00:00<?, ?it/s] 25%|██▌       | 20/80 [00:05<00:16,  3.73it/s] 50%|█████     | 40/80 [00:09<00:09,  4.08it/s] 75%|███████▌  | 60/80 [00:14<00:04,  4.19it/s]100%|██████████| 80/80 [00:19<00:00,  4.25it/s]100%|██████████| 80/80 [00:19<00:00,  4.17it/s]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.028271405671605947
    p50 = 0.028900868998346416
    p75 = 0.029428177959516563
    p90 = 0.0297220813252123
    p95 = 0.029970117973598095
    p99 = 0.031035996783767093
    mean = 0.028786215268648014
    min = 0.023887035984080283
    max = 0.03375051176478436
    stddev = 0.0012018959251338963
ttft_s
    p25 = 0.6504574517603032
    p50 = 0.7703070230782032
    p75 = 0.7725275899865665
    p90 = 0.8605480595026169
    p95 = 0.9073095369036309
    p99 = 0.9142500314628705
    mean = 0.689884057303425
    min = 0.051128369057551026
    max = 0.9147125850431621
    stddev = 0.211879682957881
end_to_end_latency_s
    p25 = 4.18792255979497
    p50 = 4.333872069953941
    p75 = 4.441300967533607
    p90 = 4.558784443652257
    p95 = 4.5874959687120285
    p99 = 4.638919534999877
    mean = 4.293092715603416
    min = 2.8255599660333246
    max = 4.711534415837377
    stddev = 0.2576919702544925
request_output_throughput_token_per_s
    p25 = 33.979491879047615
    p50 = 34.59859093610508
    p75 = 35.36870652322298
    p90 = 36.20467508324001
    p95 = 36.47148181085774
    p99 = 41.82908526293348
    mean = 34.796463710500554
    min = 29.3747083756003
    max = 41.86046182301643
    stddev = 1.553489534834863
number_input_tokens
    p25 = 463.75
    p50 = 563.5
    p75 = 635.25
    p90 = 739.3000000000002
    p95 = 826.2499999999999
    p99 = 981.4899999999996
    mean = 553.7375
    min = 175
    max = 1036
    stddev = 158.77447932463872
number_output_tokens
    p25 = 144.75
    p50 = 149.0
    p75 = 157.0
    p90 = 160.10000000000002
    p95 = 162.14999999999998
    p99 = 167.20999999999998
    mean = 149.4375
    min = 83
    max = 168
    stddev = 11.179596865538104
Number Of Errored Requests: 0
Overall Output Throughput: 623.6030794518214
Number Of Completed Requests: 80
Completed Requests Per Minute: 250.38015737086934
