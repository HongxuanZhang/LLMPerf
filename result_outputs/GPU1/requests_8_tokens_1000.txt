You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 01:42:03,909	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 01:42:03,909	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 18.83076 to 18.
2024-09-24 01:42:04,025	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:04<00:13,  1.72it/s] 50%|█████     | 16/32 [00:08<00:08,  1.92it/s] 75%|███████▌  | 24/32 [00:12<00:04,  1.99it/s]100%|██████████| 32/32 [00:16<00:00,  2.01it/s]100%|██████████| 32/32 [00:16<00:00,  1.97it/s]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.024196331030384124
    p50 = 0.02464009300969979
    p75 = 0.02504813190634278
    p90 = 0.02523766639455874
    p95 = 0.025289008013063864
    p99 = 0.02536110635838868
    mean = 0.02458677798780455
    min = 0.023574446874130288
    max = 0.025391456950113505
    stddev = 0.0005089147846434495
ttft_s
    p25 = 0.6297934735193849
    p50 = 0.685284256003797
    p75 = 0.6954044394078664
    p90 = 0.7116582138463855
    p95 = 0.7120389845222235
    p99 = 0.7123540494195185
    mean = 0.6094129005577997
    min = 0.09006825112737715
    max = 0.7124018920585513
    stddev = 0.19180561241794195
end_to_end_latency_s
    p25 = 3.5688710435060784
    p50 = 3.6663916944526136
    p75 = 3.7742314080242068
    p90 = 3.8374160130973904
    p95 = 3.86668769470416
    p99 = 3.9009668574412353
    mean = 3.6607405328031746
    min = 3.3138215688522905
    max = 3.902106426190585
    stddev = 0.14659832027028186
request_output_throughput_token_per_s
    p25 = 39.91799216623122
    p50 = 40.58045763325224
    p75 = 41.32363599142801
    p90 = 41.78604901636211
    p95 = 42.30875279687356
    p99 = 42.39484893680924
    mean = 40.68461695524274
    min = 39.37903947975887
    max = 42.41458441323714
    stddev = 0.8487375544702344
number_input_tokens
    p25 = 921.5
    p50 = 1020.0
    p75 = 1066.75
    p90 = 1220.6000000000004
    p95 = 1331.75
    p99 = 1464.6100000000001
    mean = 1012.53125
    min = 625
    max = 1486
    stddev = 173.21550708629206
number_output_tokens
    p25 = 143.0
    p50 = 149.0
    p75 = 153.75
    p90 = 160.0
    p95 = 161.0
    p99 = 161.69
    mean = 149.0
    min = 131
    max = 162
    stddev = 8.052248734199619
Number Of Errored Requests: 0
Overall Output Throughput: 293.6752220527257
Number Of Completed Requests: 32
Completed Requests Per Minute: 118.25847867894996
