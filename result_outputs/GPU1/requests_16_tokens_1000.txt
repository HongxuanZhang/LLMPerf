You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 01:43:44,155	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 01:43:44,155	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 18.83076 to 18.
2024-09-24 01:43:44,331	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/64 [00:00<?, ?it/s] 25%|██▌       | 16/64 [00:07<00:23,  2.09it/s] 50%|█████     | 32/64 [00:15<00:15,  2.09it/s] 75%|███████▌  | 48/64 [00:22<00:07,  2.17it/s]100%|██████████| 64/64 [00:29<00:00,  2.20it/s]100%|██████████| 64/64 [00:29<00:00,  2.17it/s]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.032827275598895325
    p50 = 0.03329499580825271
    p75 = 0.03397128303718221
    p90 = 0.03469139851720913
    p95 = 0.045997465284547255
    p99 = 0.05056473739019745
    mean = 0.03428883339180176
    min = 0.03120740901777139
    max = 0.05099217121934761
    stddev = 0.0040683949307921425
ttft_s
    p25 = 0.7972865097690374
    p50 = 0.8112952645169571
    p75 = 1.2300833818153478
    p90 = 1.2374084246344865
    p95 = 4.088861246558353
    p99 = 4.854122497546486
    mean = 1.143998792180355
    min = 0.09045170294120908
    max = 4.996719661168754
    stddev = 0.9877790889771884
end_to_end_latency_s
    p25 = 4.8987764994381
    p50 = 5.029636701918207
    p75 = 5.160707557632122
    p90 = 5.305061630601995
    p95 = 6.705648874887258
    p99 = 7.276638766184914
    mean = 5.133149191624398
    min = 4.407924507977441
    max = 7.605347643140703
    stddev = 0.5732605211811115
request_output_throughput_token_per_s
    p25 = 29.434915699876917
    p50 = 30.032354520980313
    p75 = 30.460078116442357
    p90 = 31.00771108174105
    p95 = 31.221300088172203
    p99 = 31.7857943636761
    mean = 29.45498981682627
    min = 19.61000368664822
    max = 32.04059478610911
    stddev = 2.519598072363811
number_input_tokens
    p25 = 919.75
    p50 = 1013.5
    p75 = 1085.25
    p90 = 1226.1000000000001
    p95 = 1312.2499999999998
    p99 = 1442.5299999999997
    mean = 1015.53125
    min = 625
    max = 1486
    stddev = 157.1251617922921
number_output_tokens
    p25 = 144.0
    p50 = 149.0
    p75 = 157.0
    p90 = 160.7
    p95 = 161.85
    p99 = 166.10999999999999
    mean = 149.96875
    min = 131
    max = 168
    stddev = 8.192076601669386
Number Of Errored Requests: 0
Overall Output Throughput: 325.8683406296343
Number Of Completed Requests: 64
Completed Requests Per Minute: 130.37449760552155
