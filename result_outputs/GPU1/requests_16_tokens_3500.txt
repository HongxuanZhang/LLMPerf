You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 01:44:23,101	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 01:44:23,102	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 18.83076 to 18.
2024-09-24 01:44:23,208	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/64 [00:00<?, ?it/s] 25%|██▌       | 16/64 [00:18<00:56,  1.18s/it] 50%|█████     | 32/64 [00:38<00:38,  1.19s/it] 75%|███████▌  | 48/64 [00:56<00:18,  1.19s/it]100%|██████████| 64/64 [01:15<00:00,  1.18s/it]100%|██████████| 64/64 [01:15<00:00,  1.18s/it]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.053202830479496234
    p50 = 0.07823271952270541
    p75 = 0.11098469258054236
    p90 = 0.13134228841158757
    p95 = 0.13381436745359046
    p99 = 0.13502657425309886
    mean = 0.08090902676781273
    min = 0.030467678570458966
    max = 0.13630373725654
    stddev = 0.035782308060436134
ttft_s
    p25 = 3.8993505277321674
    p50 = 7.75428678910248
    p75 = 11.918691805913113
    p90 = 14.982694787578659
    p95 = 15.289545080717652
    p99 = 15.550440733381548
    mean = 7.892345620879496
    min = 0.2802623310126364
    max = 15.59187562786974
    stddev = 5.274933906122127
end_to_end_latency_s
    p25 = 8.29298993840348
    p50 = 12.348357034032233
    p75 = 16.03350075497292
    p90 = 18.6553081483813
    p95 = 18.7510816266411
    p99 = 19.155471704092342
    mean = 12.011416012664995
    min = 4.494328364962712
    max = 19.222155881114304
    stddev = 5.112951733874828
request_output_throughput_token_per_s
    p25 = 9.015709682314512
    p50 = 12.939147495249399
    p75 = 19.49729119726618
    p90 = 30.68878187623331
    p95 = 31.755102909590317
    p99 = 32.279831332789044
    mean = 15.875008559578262
    min = 7.33644261718378
    max = 32.82007565592908
    stddev = 8.69349963902865
number_input_tokens
    p25 = 3419.75
    p50 = 3513.5
    p75 = 3585.25
    p90 = 3726.1
    p95 = 3812.2499999999995
    p99 = 3942.5299999999997
    mean = 3515.53125
    min = 3125
    max = 3986
    stddev = 157.1251617922921
number_output_tokens
    p25 = 144.0
    p50 = 149.0
    p75 = 156.25
    p90 = 160.7
    p95 = 161.85
    p99 = 166.10999999999999
    mean = 149.71875
    min = 131
    max = 168
    stddev = 8.10931024800585
Number Of Errored Requests: 0
Overall Output Throughput: 126.57754737120253
Number Of Completed Requests: 64
Completed Requests Per Minute: 50.726130443061756
