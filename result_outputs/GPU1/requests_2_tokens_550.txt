You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 01:39:21,594	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 01:39:21,594	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 18.83076 to 18.
2024-09-24 01:39:21,699	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:03<00:10,  1.70s/it] 50%|█████     | 4/8 [00:05<00:05,  1.40s/it] 75%|███████▌  | 6/8 [00:08<00:02,  1.36s/it]100%|██████████| 8/8 [00:10<00:00,  1.32s/it]100%|██████████| 8/8 [00:10<00:00,  1.37s/it]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.016494844363520125
    p50 = 0.016687006219713522
    p75 = 0.017016902181502702
    p90 = 0.017368901279806396
    p95 = 0.017387415384168648
    p99 = 0.01740222666765845
    mean = 0.016808533398587622
    min = 0.01645415840865253
    max = 0.0174059294885309
    stddev = 0.0003946463972836925
ttft_s
    p25 = 0.03863877756521106
    p50 = 0.050568135106004775
    p75 = 0.09837046527536586
    p90 = 0.10915174330584705
    p95 = 0.11291098510846495
    p99 = 0.11591837855055928
    mean = 0.06421556152054109
    min = 0.024938887916505337
    max = 0.11667022691108286
    stddev = 0.03660588859282835
end_to_end_latency_s
    p25 = 2.3616440255427733
    p50 = 2.450922782998532
    p75 = 2.5196310250321403
    p90 = 2.5633413400501013
    p95 = 2.6080305335344747
    p99 = 2.6437818883219735
    mean = 2.4524120782734826
    min = 2.2994234890211374
    max = 2.652719727018848
    stddev = 0.11707992100778986
request_output_throughput_token_per_s
    p25 = 58.76340230900958
    p50 = 59.92685457241419
    p75 = 60.6179978231352
    p90 = 60.71468843286901
    p95 = 60.740659393635084
    p99 = 60.761436162247946
    mean = 59.514232564169845
    min = 57.44481618017576
    max = 60.766630354401165
    stddev = 1.3804171918059271
number_input_tokens
    p25 = 502.5
    p50 = 534.5
    p75 = 586.0
    p90 = 710.8
    p95 = 838.8999999999999
    p99 = 941.3799999999999
    mean = 585.25
    min = 472
    max = 967
    stddev = 160.4767450887689
number_output_tokens
    p25 = 139.5
    p50 = 143.5
    p75 = 153.0
    p90 = 155.4
    p95 = 158.2
    p99 = 160.44
    mean = 146.0
    min = 136
    max = 161
    stddev = 8.750510189207745
Number Of Errored Requests: 0
Overall Output Throughput: 106.71088384728051
Number Of Completed Requests: 8
Completed Requests Per Minute: 43.853787882444045
