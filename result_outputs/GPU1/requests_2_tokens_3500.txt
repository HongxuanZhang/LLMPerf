You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 01:40:00,857	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 01:40:00,857	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 18.83076 to 18.
2024-09-24 01:40:01,995	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:03<00:11,  1.90s/it] 50%|█████     | 4/8 [00:07<00:06,  1.73s/it] 75%|███████▌  | 6/8 [00:10<00:03,  1.79s/it]100%|██████████| 8/8 [00:14<00:00,  1.79s/it]100%|██████████| 8/8 [00:14<00:00,  1.79s/it]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.023084725446887718
    p50 = 0.02317085020222056
    p75 = 0.023434039432955526
    p90 = 0.023741911234552902
    p95 = 0.02395690649304192
    p99 = 0.024128902699833133
    mean = 0.023237670810136696
    min = 0.022309047195108997
    max = 0.024171901751530937
    stddev = 0.000525746842726849
ttft_s
    p25 = 0.2996806452283636
    p50 = 0.4033542249817401
    p75 = 0.5628667185083032
    p90 = 0.6129161084769293
    p95 = 0.6139590192004107
    p99 = 0.6147933477791958
    mean = 0.42503451387165114
    min = 0.2454028301872313
    max = 0.615001929923892
    stddev = 0.15782368711283
end_to_end_latency_s
    p25 = 3.1408629152574576
    p50 = 3.2962572346441448
    p75 = 3.5519144987338223
    p90 = 3.622044587600976
    p95 = 3.671129327896051
    p99 = 3.710397120132111
    mean = 3.2691867214161903
    min = 2.4628847658168525
    max = 3.720214068191126
    stddev = 0.39249623591425015
request_output_throughput_token_per_s
    p25 = 42.66466712234261
    p50 = 43.154105303856056
    p75 = 43.315646131958935
    p90 = 43.84794283660756
    p95 = 44.333972459894305
    p99 = 44.72279615852371
    mean = 43.00357468950277
    min = 41.008820794951745
    max = 44.82000208318106
    stddev = 1.066787817210082
number_input_tokens
    p25 = 3452.5
    p50 = 3484.5
    p75 = 3536.0
    p90 = 3660.7999999999997
    p95 = 3788.8999999999996
    p99 = 3891.38
    mean = 3535.25
    min = 3422
    max = 3917
    stddev = 160.4767450887689
number_output_tokens
    p25 = 138.25
    p50 = 141.5
    p75 = 153.0
    p90 = 155.4
    p95 = 158.2
    p99 = 160.44
    mean = 140.75
    min = 101
    max = 161
    stddev = 18.21106414086934
Number Of Errored Requests: 0
Overall Output Throughput: 78.67353172319515
Number Of Completed Requests: 8
Completed Requests Per Minute: 33.53756236867999
