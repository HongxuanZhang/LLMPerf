You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 20:46:42,340	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 20:46:42,340	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 61.43999 to 61.
2024-09-24 20:46:42,502	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/80 [00:00<?, ?it/s] 25%|██▌       | 20/80 [00:04<00:14,  4.13it/s] 50%|█████     | 40/80 [00:09<00:09,  4.38it/s] 75%|███████▌  | 60/80 [00:13<00:04,  4.48it/s]100%|██████████| 80/80 [00:17<00:00,  4.55it/s]100%|██████████| 80/80 [00:17<00:00,  4.48it/s]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.026685570115467058
    p50 = 0.027089609251820152
    p75 = 0.0274141566719674
    p90 = 0.027864261225266397
    p95 = 0.02793196716328119
    p99 = 0.028086391596541416
    mean = 0.027060003952176837
    min = 0.025677816487413588
    max = 0.028253788668360164
    stddev = 0.000568270781860995
ttft_s
    p25 = 0.6932126254687319
    p50 = 0.8278506229980849
    p75 = 0.8539317065151408
    p90 = 0.8563318938133306
    p95 = 0.8580798751470865
    p99 = 0.8810546132770831
    mean = 0.7308324664241809
    min = 0.09408165601780638
    max = 0.904259623028338
    stddev = 0.22677003346137903
end_to_end_latency_s
    p25 = 3.962159950766363
    p50 = 4.061716460477328
    p75 = 4.199402411497431
    p90 = 4.272839319618652
    p95 = 4.315188544927514
    p99 = 4.350669102947577
    mean = 4.063151389988343
    min = 3.643444340035785
    max = 4.351215004047845
    stddev = 0.16605897990962698
request_output_throughput_token_per_s
    p25 = 36.47466531799584
    p50 = 36.91162696622748
    p75 = 37.470306114129905
    p90 = 37.929613850995025
    p95 = 38.3057621017108
    p99 = 38.76168503967806
    mean = 36.96803477470499
    min = 35.39047970006717
    max = 38.940444243019755
    stddev = 0.7798866224137941
number_input_tokens
    p25 = 913.75
    p50 = 1013.5
    p75 = 1085.25
    p90 = 1189.3000000000002
    p95 = 1276.2499999999998
    p99 = 1431.4899999999996
    mean = 1003.7375
    min = 625
    max = 1486
    stddev = 158.77447932463872
number_output_tokens
    p25 = 145.0
    p50 = 149.5
    p75 = 157.0
    p90 = 160.10000000000002
    p95 = 162.14999999999998
    p99 = 167.20999999999998
    mean = 150.275
    min = 130
    max = 168
    stddev = 8.270697091524198
Number Of Errored Requests: 0
Overall Output Throughput: 673.6571656074411
Number Of Completed Requests: 80
Completed Requests Per Minute: 268.96975502542983
