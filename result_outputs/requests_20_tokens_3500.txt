You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 01:47:01,488	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 01:47:01,489	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 18.83076 to 18.
2024-09-24 01:47:01,609	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/80 [00:00<?, ?it/s] 25%|██▌       | 20/80 [00:23<01:11,  1.20s/it] 50%|█████     | 40/80 [00:47<00:47,  1.18s/it] 75%|███████▌  | 60/80 [01:11<00:23,  1.19s/it]100%|██████████| 80/80 [01:34<00:00,  1.17s/it]100%|██████████| 80/80 [01:34<00:00,  1.18s/it]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.06380507242023772
    p50 = 0.09473439003901515
    p75 = 0.13554772400502665
    p90 = 0.16118787047940333
    p95 = 0.16507073440350256
    p99 = 0.17658248608321409
    mean = 0.09615774670405629
    min = 0.03052034745528515
    max = 0.18144465679696833
    stddev = 0.04512159055996461
ttft_s
    p25 = 5.355596424953546
    p50 = 10.15299099800177
    p75 = 15.345507423917297
    p90 = 19.45888597269077
    p95 = 19.971428184921386
    p99 = 20.275129902835467
    mean = 10.199601660622283
    min = 0.3181765889748931
    max = 20.624128887197003
    stddev = 6.638890509711546
end_to_end_latency_s
    p25 = 9.3161672737333
    p50 = 14.515680267941207
    p75 = 19.409370533947367
    p90 = 23.341538271610624
    p95 = 23.58872344169067
    p99 = 23.728513910192994
    mean = 14.371425832936074
    min = 4.320720345014706
    max = 23.80923922196962
    stddev = 6.560508371152762
request_output_throughput_token_per_s
    p25 = 7.377427119157874
    p50 = 10.555581799508364
    p75 = 15.672233963082057
    p90 = 30.819273588583236
    p95 = 31.854972681842554
    p99 = 32.41207514837025
    mean = 14.06324294863245
    min = 5.511239921446561
    max = 32.761749509500945
    stddev = 8.814565245759454
number_input_tokens
    p25 = 3413.75
    p50 = 3513.5
    p75 = 3585.25
    p90 = 3689.3
    p95 = 3776.25
    p99 = 3931.49
    mean = 3503.7375
    min = 3125
    max = 3986
    stddev = 158.77447932463872
number_output_tokens
    p25 = 145.0
    p50 = 149.5
    p75 = 157.0
    p90 = 160.10000000000002
    p95 = 162.14999999999998
    p99 = 167.20999999999998
    mean = 150.275
    min = 130
    max = 168
    stddev = 8.270697091524198
Number Of Errored Requests: 0
Overall Output Throughput: 127.54742614339345
Number Of Completed Requests: 80
Completed Requests Per Minute: 50.92560684480856
