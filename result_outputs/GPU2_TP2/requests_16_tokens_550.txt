You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 20:19:49,436	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 20:19:49,437	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 61.43999 to 61.
2024-09-24 20:19:49,595	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/64 [00:00<?, ?it/s] 25%|██▌       | 16/64 [00:04<00:14,  3.36it/s] 50%|█████     | 32/64 [00:08<00:08,  3.66it/s] 75%|███████▌  | 48/64 [00:12<00:04,  3.81it/s]100%|██████████| 64/64 [00:16<00:00,  4.09it/s]100%|██████████| 64/64 [00:16<00:00,  3.92it/s]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.02386482916061075
    p50 = 0.02540049852047952
    p75 = 0.026077910058234552
    p90 = 0.026398841983796953
    p95 = 0.0264795285461915
    p99 = 0.027360403113321993
    mean = 0.024883096329178015
    min = 0.02150132125446006
    max = 0.028138829590917555
    stddev = 0.0015713393968000036
ttft_s
    p25 = 0.9754636634897906
    p50 = 1.4558741939836182
    p75 = 1.4785589102975791
    p90 = 1.5320890615228564
    p95 = 1.5327691295038677
    p99 = 1.5335066651634406
    mean = 1.19941560596817
    min = 0.09965595504036173
    max = 1.533687638991978
    stddev = 0.41679673782884136
end_to_end_latency_s
    p25 = 3.4576480627438286
    p50 = 3.828097567980876
    p75 = 3.9196102834830526
    p90 = 4.014411480014678
    p95 = 4.039118246894214
    p99 = 4.096290919882595
    mean = 3.715930151873181
    min = 3.05421450099675
    max = 4.096696231048554
    stddev = 0.2878197585086781
request_output_throughput_token_per_s
    p25 = 38.343245403973086
    p50 = 39.36643704722457
    p75 = 41.906230265291285
    p90 = 44.69599892415684
    p95 = 45.24781583260584
    p99 = 46.28906230103258
    mean = 40.34694167760533
    min = 35.3394870655548
    max = 46.5036697811284
    stddev = 2.6763450688376738
number_input_tokens
    p25 = 469.75
    p50 = 563.5
    p75 = 635.25
    p90 = 776.1
    p95 = 862.2499999999998
    p99 = 992.5299999999999
    mean = 565.53125
    min = 175
    max = 1036
    stddev = 157.1251617922921
number_output_tokens
    p25 = 143.75
    p50 = 149.0
    p75 = 157.0
    p90 = 160.7
    p95 = 161.85
    p99 = 166.10999999999999
    mean = 149.453125
    min = 113
    max = 168
    stddev = 9.395929019679594
Number Of Errored Requests: 0
Overall Output Throughput: 585.3750723512259
Number Of Completed Requests: 64
Completed Requests Per Minute: 235.00682465537972
