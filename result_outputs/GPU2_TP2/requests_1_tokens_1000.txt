You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2024-09-24 20:16:16,174	WARNING utils.py:580 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2024-09-24 20:16:16,175	WARNING utils.py:592 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 61.43999 to 61.
2024-09-24 20:16:16,422	INFO worker.py:1786 -- Started a local Ray instance.
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:02<00:07,  2.34s/it] 50%|█████     | 2/4 [00:03<00:03,  1.93s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.72s/it]100%|██████████| 4/4 [00:07<00:00,  1.66s/it]100%|██████████| 4/4 [00:07<00:00,  1.76s/it]
\Results for token benchmark for llama2-7b-chat queried with the openai api.

inter_token_latency_s
    p25 = 0.010197097570079933
    p50 = 0.010204471064984622
    p75 = 0.01022019196121731
    p90 = 0.010246529392272647
    p95 = 0.010255308535957759
    p99 = 0.010262331850905849
    mean = 0.01021281846631262
    min = 0.01017824405563837
    max = 0.010264087679642871
    stddev = 3.6357723277050276e-05
ttft_s
    p25 = 0.10576522552582901
    p50 = 0.10656713001662865
    p75 = 0.10764430175186135
    p90 = 0.10815466970670969
    p95 = 0.10832479235832579
    p99 = 0.10846089047961868
    mean = 0.1068423972610617
    min = 0.10574041400104761
    max = 0.1084949150099419
    stddev = 0.0013362180365884522
end_to_end_latency_s
    p25 = 1.5351999814884039
    p50 = 1.55949267197866
    p75 = 1.5819039767375216
    p90 = 1.6188030943041667
    p95 = 1.631102800159715
    p99 = 1.6409425648441538
    mean = 1.5576112862472655
    min = 1.4680572950164787
    max = 1.6434025060152635
    stddev = 0.07163434902202168
request_output_throughput_token_per_s
    p25 = 97.827519620698
    p50 = 97.97809130439671
    p75 = 98.04884021494122
    p90 = 98.1570855620895
    p95 = 98.19316734447226
    p99 = 98.22203277037848
    mean = 97.89826853124251
    min = 97.40764238932162
    max = 98.22924912685502
    stddev = 0.34796109498658917
number_input_tokens
    p25 = 986.25
    p50 = 1009.5
    p75 = 1036.0
    p90 = 1045.0
    p95 = 1048.0
    p99 = 1050.4
    mean = 1012.75
    min = 981
    max = 1051
    stddev = 33.74783943701681
number_output_tokens
    p25 = 150.5
    p50 = 153.0
    p75 = 155.0
    p90 = 158.6
    p95 = 159.8
    p99 = 160.76
    mean = 152.5
    min = 143
    max = 161
    stddev = 7.371114795831994
Number Of Errored Requests: 0
Overall Output Throughput: 86.8033457461086
Number Of Completed Requests: 4
Completed Requests Per Minute: 34.15213603125584
